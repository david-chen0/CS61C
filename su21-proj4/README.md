# numc
- How many hours did you spend on the following tasks?
  - Task 1 (Matrix functions in C): 5
  - Task 2 (Writing the Python-C interface): 0
  - Task 3 (Speeding up matrix operations): 20
- Was this project interesting? What was the most interesting aspect about it?
  - <b>I liked the speedup section of this project. Though some speedups were straight forward, such as adding pragma parallel to multiple loops, some of the solutions we had took us quite a while to reach. For example, for the matrix power, we changed the algorithm so that instead of O(N) calls to matrix multiply, it now made only O(log(N)) calls to matrix multiply, which greatly increased its speed. We did this by using an algorithm that continually squared our holder matrix. We first convert the power argument, which is given in decimal form, to binary form. Then, if the ith binary digit is 1, we square the matrix and then multiply by the original matrix in ith iteration. Otherwise, we just square the matrix and move onto the next iteration. Though this was very helpful in speeding up matrix power, we found out that our main bottleneck, however, was matrix multiplication, since most calls used matrix multiplication and the matrix power algorithm used matrix multiplication as a black box. Therefore, we tested multiple speedup methods for matrix multiplication and decided on using pragma omp parallels where it would benefit us, transposing the second matrix so that the cache accesses within the loops would have a higher hit percentage, and using intrinsics to speed up the inner loop. For pragma omps, we found out that implementing parallelism in certain spots was extremely helpful, such as in the main loop for matrix traversal and dot product when matrix dimensions are greatrer than 10 by 10. However, we couldn't simply use parallelism and loop unrolling for every loop, as it would actually slow down our operation for certain loops. For example, when the matrix dimensions are less than 10 by 10, parallelism would not have great enough an effect to account for its constant factor cost. Additionally, for the main loops where we iterate through the rows and columns, stacking two parallel loops would actually decrease performance, since the threads would take too long to handle all the cases. Therefore, we had to be cautious with how we handled parallelism and loop unrolling for matrix multiply. For other functions such as add, abs, and sub, we felt that adding parallelism was sufficient for our speedup, as unrolling likely would not have made a difference and using intrinsics actually decreased our runtime.</b>
- What did you learn?
  - <b>I realized the importance of seemingly small changes in our methods. For example, a simple change such as transposing the second matrix gave us a 3x speedup. Though this only increased the total cache hit rate by about 40%, since we were getting 0 cache hits earlier and are now getting 2 or more cache hits(depending on the size of the caches) every 5 accesses, it had a huge effect on the overall runtime of our program. Additionally, the difference between using parallel loops and not using parallel loops was way greater than what I thought, as adding parallelism to loops gave us a 5x speedup on large cases. However, on tests that worked with small dimensions or small powers, the parallelism actually decreased our speedup, which we combated by adding an if statement so that we only use parallelism if the matrix parameters were big enough. Furthermore, we had to think about whether parallelism would actually benefit the loop. For example, on the matrix multiplication loops, there would definitely be a huge benefit, as the loops are disjoint and would not provide a data race. However, for smaller or more involved operations such as abs or pow, parallelism would not have a positive effect, as it either doesn't run for long enough or has to spend time and resources preventing a data race. Outside of employing parallelism, there were some changes that didn't hold the effect that I expected. For example, I expected loop unrolling to have a much bigger impact on the performance, but it had a minimal impact or even decreased the performance in some cases. Though this was explained at the end of task 3, I still had expected a bigger impact, given the experience from labs. In the end, the amount of trials we spent with possible pragma omp loop combinations, what dimensions to use pragma for, and where to use loop unrolling gave me deeper insight into when parallelism and certain other changes would be actually helpful, and when it would be detrimental.</b>
- Is there anything you would change?
  - <b>I personally wasn't a fan of implementing the numc file, as it was simply working through toilsome logic cases and referencing the Python documentations for how to work with PyObjects and more. I would have put more of an emphasis on the speedup and not have the numc file be a task. For example, one idea I had for the matrix power algorithm was to diagonalize the matrix, if diagonalizable, and take the power of the result. This would likely have gotten a 1500x speedup for higher dimensions that are diagonalizable, as diagonalizing the matrix takes a small amount of time compared to all the matrix multiplications being performed in the original algorithm. Additionally, taking the power of a diagonalization would simply require taking the power of each individual number on the diagonal of the diagonal matrix, which takes a negligible amount of time. Therefore, this would have been an extremely powerful matrix pow algorithm. However, there was no reason for me to implement such an algorithm, as we could meet the 900x with our original algorithm. I would recommend putting some sort of incentive to reach 1500x, or whatever the speedup is with the diagonalization implementation, to further motivate the speedup section. I would also have liked to have had more freedom over choosing our matrix struct, as I felt restricted by only staying to a few variables. For example, in my implementation, I added two variables to the matrix struct, ptr_ref_cnt and par_data. These were pointers to the original or parent matrix's ref_cnt and data, which would ensure that we wouldn't have incorrect data somehow from allocate or deallocate operations, or even from slicing operations.</b>
